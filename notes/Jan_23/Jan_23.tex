\documentclass{article}


\usepackage[margin=0.6in]{geometry}
\usepackage{amssymb, amsmath, amsfonts}
\usepackage{tabularx}
\usepackage{arydshln}
\usepackage{mathtools}
\usepackage{cancel}
\usepackage{physics}
\usepackage{pgf}
\usepackage{enumerate}
\usepackage{placeins}
\usepackage{enumitem}
\usepackage{nth}
\usepackage{array}
\usepackage{tikz}
\usetikzlibrary{arrows,automata}
\usepackage{nicefrac}
\usepackage{pgfplots}
\newcommand{\enth}{$n$th}
\newcommand{\Rl}{\mathbb{R}}
\newcommand{\Cx}{\mathbb{C}}
\newcommand{\sgn}[1]{\text{sgn}\qty[#1]}
\newcommand{\ran}[1]{\text{ran}\qty[#1]}
\newcommand{\E}{\varepsilon}
\newcommand{\qiq}{\qquad \implies \qquad}
\newcommand{\half}{\nicefrac{1}{2}}
\newcommand{\third}{\nicefrac{1}{3}}
\newcommand{\quarter}{\nicefrac{1}{4}}
\newcommand{\f}[3]{#1\ :\ #2 \rightarrow #3}
\newcommand{\Dx}{\Delta x}
\newcommand{\Dt}{\Delta t}
\newcommand{\hot}{\text{h.o.t.}}

\newcommand{\tridsym}[3]{
    \qty(\begin{array}{ccccc}
                    #1 & #2 & & & \\
                    #3 & #1 & #2 & & \\
                    & \ddots & \ddots & \ddots &  \\
                    & & #3 & #1 & #2 \\
                    & & & #3 & #1
                \end{array})
}


\DeclareMathOperator*{\esssup}{\text{ess~sup}}

\title{MAT 228B Notes}
\author{Sam Fleischer}
\date{January 23, 2016}

\begin{document}
    \maketitle

    \section{Review}
        \begin{itemize}
            \item Trapezoidal Rule
            \begin{align*}
                \frac{y^{n+1} - y^n}{\Dt} = \frac{1}{2}\qty(f(y^n) + f(y^{n+1}))
            \end{align*}
            This is actually a centered difference around the half-time level.
            \begin{align*}
                \frac{y^{n+1} - y^n}{\Dt} = \frac{1}{2}\qty(f(y^n) + f(y^{n+1})) = f(y^{n+\half}) + \order{(\Dt)^2}
            \end{align*}
            using Taylor Series.
            \item BDF-2
            \begin{align*}
                \frac{3y^{n+1} - 4y^n + y^{n-1}}{2\Dt} = f(y^{n+1})
            \end{align*}
            \item Both of these are A-Stable (both of their regions of absolute stability contain the left half of the complex plane, i.e.~if the solution \emph{should} decay, it does, independent of timestep).
        \end{itemize}

    \section{Region of Absolute Stability for the Trapezoidal Rule}
        \begin{itemize}
            \item
            \begin{align*}
                \frac{y^{n+1} - y^n}{\Dt} &= \frac{\lambda}{2}\qty(y^n + y^{n+1}) \\
                \qty(1 - \frac{z}{2})y^{n+1} &= \qty(1 + \frac{z}{2})y^n \\
                y^{n+1} &= \qty(\frac{2 + z}{2 - z})y^n
            \end{align*}
            So the region of absolute stability is defined by
            \begin{align*}
                \abs{\frac{2 + z}{2 - z}} &< 1 \\
                \abs{2 + z} &< \abs{2 - z}
            \end{align*}
            This is the left half of the complex plane because the above says the distance to $-2$ must be less than the distance to $2$.  This is the minimal amount you need to be A-Stable.
            \item Trapezoidal Rule for Diffusion with standard 3-point second order (1D) spacial discretization is called ``Crank-Nicolson.''  The scheme is 2nd order in space and time and is unconditionally stable (pick any timestep you want).
        \end{itemize}

    \section{Runga-Kutta Methods vs. Linear Multistep Methods}
        Runga-Kutta are single-step multi-stage methods, whereas Linear Multistep Methods use information from past time.
        
        \subsection{Runga-Kutta Methods}
            \begin{itemize}
                \item 2nd Order ``Improved Euler'' Runga-Kutta Method
                \begin{align*}
                    y^* = y^n + \Dt f(y^n) \\
                    y^{n+1} = y^n + \frac{Dt}{2}\qty(f(y^n) + y(y^*))
                \end{align*}
                Note we are only using $y^n$, not any other past timesteps.  We are using stages to get $y^{n+1}$.
                \item General form of an $r$-stage RK Method:
                \begin{align*}
                    y' &= f(t,y) \\
                    y_i^* &= y^n + \Dt\sum_{j=1}^r A_{ij}f(t_n + c_j\Dt,y_j^*) \qquad i = 1,\dots,r \\
                    y^{n+1} &= y^n + \Dt\sum_{j=1}^r b_jf(t_n + c_j\Dt,y_j^*)
                \end{align*}
                This is an implicit Runga-Kutta scheme.. Explicit RK schemes sum from $j=1$ to $i-1$ rather than $r$.
                \item $A$ is called the Runga-Kutta Matrix, $b$ are called the RK weights, and $c$ are called the RK nodes.  These are expressed it Butcher Tables in the form
                \begin{align*}
                    \begin{array}{l|c}
                        \underline{c} & \ \ \ A\ \ \ \\[0.3cm]\hline
                        & \underline{b}^T
                    \end{array}
                \end{align*}
                \begin{align*}
                    y_1^* &= y^n \\
                    y_2^* = y^n + \Dt f(t_n,y_1^*) \\
                    y^{n+1} = y^n + \frac{\Dt}{2}\qty(f(t_n,y^*) + f(t_n + \Dt,y_2^*))
                \end{align*}
                gives the Butcher Table
                \begin{align*}
                    \begin{array}{c|cc}
                        0 & 0 & 0 \\
                        1 & 1 & 0 \\\hline
                        & \half & \half
                    \end{array}
                \end{align*}
                \item Classical RK4 Scheme
                \begin{align*}
                    \begin{array}{c|cccc}
                        0 & 0 & 0 & 0 & 0\\
                        \half & \half & 0 & 0 & 0\\
                        \half & 0 & \half & 0 & 0 \\
                        1 & 0 & 0 & 1 & 0 \\\hline
                        & \nicefrac{1}{6} & \nicefrac{1}{3} & \nicefrac{1}{3} & \nicefrac{1}{6}
                    \end{array}
                \end{align*}
                If $A$ is strictly lower-triangular, it is an explicit time method.  It doesn't have to be, though, and is very high-order accurate in time, but it is very expensive.
                \item DIRK are ``diagonally implicit'' RK methods, which include nonzero elements on the diagonals:
                \begin{itemize}
                    \item TR-DBF2 Method
                    \begin{align*}
                        y^* &= y^n + \frac{\Dt}{4}\qty(f(y^n) + f(y^*)) \\
                        y^{n+1} &= \frac{1}{3}\qty(4y^* - y^n + \Dt f(y^{n+1}))
                    \end{align*}
                    This is a combination of the Trapezoidal Rule and BDF2.  This tries to blend the best of both worlds.  The Butcher Table is
                    \begin{align*}
                        \begin{array}{c|ccc}
                            0 & 0 & 0 & 0 \\
                            \half & \quarter & \quarter & 0 \\
                            1 & \third & \third & \third \\\hline
                            & \third & \third & \third 
                        \end{array}
                    \end{align*}
                \end{itemize}
            \end{itemize}
        \subsection{Linear Multistep Methods}
            \begin{itemize}
                \item ``Adams Bashforth 2'' or ``AB2''
                \begin{align*}
                    \frac{y^{n+1} - y^n}{\Dt} = \frac{1}{2}\qty(3f(y^n) - f(y^{n-1}))
                \end{align*}
                This uses data at $y^n$ and $y^{n-1}$ to get $y^{n+1}$.  This is an extrapolation in time.
                \item BDF-2 is an implicit time method.
                \item General form of an $r$-step Method
                \begin{align*}
                    \sum_{j=0}^r\alpha_jy^{n+j} = \Dt\sum_{j=0}^r\beta_jf(y^{n+j})
                \end{align*}
                \item The Adams Methods
                \begin{itemize}
                    \item \begin{align*}
                        y^{n+r} - y^{n+r-1} = \Dt\sum_{j=0}^r\beta_jf(y^{n+j})
                    \end{align*}
                    \item If $\beta_r = 0$, this is called Adams-Bashforth (eg. Forward Euler is an AB method)
                    \begin{align*}
                        y^{n+1} - y^n = \Dt f(y^n)
                    \end{align*}
                    and AB2 is
                    \begin{align*}
                        y^{n+1} - y^n = \Dt\frac{1}{2}\qty(3f(y^n) - f(y^{n-1}))
                    \end{align*}
                    \item If $\beta_r \neq 0$, Adams Moulton Method implicit time (e.g.~trapezoidal rule)
                    \begin{align*}
                        y^{n+1} - y^n = \Dt\frac{1}{2}\qty(f(y^n) + f(y^{n+1}))
                    \end{align*}
                \end{itemize}
                \item BDF-Backward (difference formulas)
                \begin{align*}
                    \sum_{j=0}^r\alpha_jy^{n+j} = \Dt\beta_rf(y^{n+r})
                \end{align*}
                These are good for stiff equations.  Backward Euler is BDF1.  High order BDFs are great if you have lots of eigenvalues clustered along the negative real axis.
            \end{itemize}



\end{document}



















